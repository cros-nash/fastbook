{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models fit functions to data\n",
    "\n",
    "We start off with an infinitely flexible function and get it to recognize patterns in input data\n",
    "\n",
    "Let's start off with a quadratic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): return 3*x**2 + 2*x + 1\n",
    "\n",
    "plot_function(f, \"$3x^2 + 2x + 1$\") # DOLLAR SIGNS LETS US WRITE MATH EQUATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function that makes creating quadratics easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad(a, b, c, x): return a*x**2 + b*x + c\n",
    "\n",
    "quad(3,2,1, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "# Partial application of quad where a,b,c are fixed values\n",
    "def mk_quad(a,b,c): return partial(quad, a,b,c)\n",
    "f = mk_quad(3,2,1)\n",
    "f(1.5)  # Now only value we have to pass is x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to create some data that matches the shape of the function:\n",
    "- adding noise because in real life this is the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal,seed,uniform\n",
    "np.random.seed(42)\n",
    "# sets the seed so we get the same random numbers\n",
    "\n",
    "def noise(x, scale): return normal(scale=scale, size=x.shape)\n",
    "# normal() creates normally distributed random numbers\n",
    "\n",
    "def add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-2, 2, steps=20)[:,None]\n",
    "# creates a tensor (vector) from -2 to 2 with 20 steps\n",
    "\n",
    "y = add_noise(f(x), 0.3, 1.5)\n",
    "# f(x) with random noise added to it\n",
    "\n",
    "plt.scatter(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that we're going to reconstruct the original equation, find the one which matches this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(a=1.5, b=1.5, c=1.5)\n",
    "def plot_quad(a, b, c):\n",
    "    plt.scatter(x, y)\n",
    "    plot_function(mk_quad(a, b, c), min=-2, max=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's supposed to happen is that we could manually change the parameters ourselves and eyeball it. \n",
    "\n",
    "That's not the best approach because so want a way to see how far off we are from the true function.\n",
    "\n",
    "So we're going to implement a loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(preds, acts): return ((preds-acts)**2).mean()\n",
    "# MEAN SQUARED ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(a=1.5, b=1.5, c=1.5)\n",
    "def plot_quad(a,b,c):\n",
    "    f = mk_quad(a,b,c)\n",
    "    plt.scatter(x,y)\n",
    "    loss = mse(f(x),y)\n",
    "    plot_function(f, title=f\"MSE: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way of changing the parameters is still manual. BUT, now we have a way of actually knowing how far we are from the true function (the lower the loss score the better our prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUTOMATE PARAMETER OPTIMIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could either continue making tweaks to the weights repeatedly and prioritize the changes that lead to a lower score.\n",
    "\n",
    "This is hella slow.\n",
    "\n",
    "Instead we should use derivatives (remember we're trying to get to the bottom of the curve where the weights are so close to the true parameters that a small change doesn't lead to a drastic increase in loss score)\n",
    "\n",
    "We need a function to tell us: if input increase does output increase or decrease and by how much.\n",
    "\n",
    "PyTorch can automatically do this for us :)\n",
    "\n",
    "First thing we need is a function that takes the coefficients of the quadratic a, b, and c as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad_mse(params):\n",
    "    f = mk_quad(*params)    # star is used to pass list into mk_quad as separate inputs\n",
    "    return mse(f(x), y)     # returns mean squared errors of predictions against actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in the coefficients of the quadratics and returns the loss\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_mse([1.5, 1.5, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our MSE is 5.8336\n",
    "\n",
    "It says its a tensor, which means it doesn't just work with numbers but also lists, or vectors of numbers (1d tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to create parameters a, b, and c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = torch.tensor([1.5,1.5,1.5])   # All parameters are put into a rank-1-tensor\n",
    "abc.requires_grad_()                # tell PyTorch that we want gradient calculated\n",
    "                                    # for these numbers when used in a calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's use it in a calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = quad_mse(abc)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grad_fn at the end tells us, if we wanted to, PyTorch knows how to calculate the gradients for our inputs. \n",
    "\n",
    "To tell PyTorch to do it, we use the backward() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now abc has an attribute called grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us:\n",
    "- Increasing a will lead to lower loss      (also the biggest change)\n",
    "- Incerasing b will lead to a lower loss    (not as much as a)\n",
    "- Increasing c will lead to a lower loss    (lowest change of the three)\n",
    "\n",
    "Thus, we should increase all of them to lower the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # abc is being used in a function but we don't want gradients to be calculated\n",
    "    \n",
    "    abc -= abc.grad*0.01\n",
    "    # only change parameters by a fraction of its respective gradient\n",
    "\n",
    "    loss = quad_mse(abc)\n",
    "    # calculate loss again\n",
    "    \n",
    "print(f'loss={loss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now automate it so that we continue to decrease the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    loss = quad_mse(abc)                        # calculate loss\n",
    "    loss.backward()                             # calculate gradients\n",
    "    with torch.no_grad(): abc -= abc.grad*0.01  # update parameters\n",
    "    print(f'step={i}; loss={loss:.2f}')         # print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now have some coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most basic type of optimzer using gradient descent\n",
    "\n",
    "THIS IS THE FOUNDATION OF HOW WE CREATE PARAMETERS\n",
    "\n",
    "So, what is the mathematical funciton we are finding those parameters for?\n",
    "\n",
    "We can't just use quadratics because its unlikely that this is the case for complex problems\n",
    "\n",
    "We can create an infinitely flexible function from rectified linear units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectified_linear(m,b,x):\n",
    "    y = m*x+b                   # LINEAR FUNCTION\n",
    "    return torch.clip(y, 0.)    # Takes output y and turns anything below 0 to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function(partial(rectified_linear, 1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make this plot function interactive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(m=1.5, b=1.5)\n",
    "def plot_relu(m, b):\n",
    "    plot_function(partial(rectified_linear, m,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could take this rectified linear function and create a double ReLu, which adds up two rectified linear functions together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_relu(m1,b1,m2,b2,x):\n",
    "    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(m1=-1.5, b1=-1.5, m2=1.5, b2=1.5)\n",
    "def plot_double_relu(m1, b1, m2, b2):\n",
    "    plot_function(partial(double_relu, m1,b1,m2,b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could add as many ReLus together as we want, so we can have an arbitrarily squiggly function and with enough ReLUs, we can match it as close as we want\n",
    "\n",
    "Imagine an audio waveform, we could use millions of ReLUs to add together to almost exactly match it\n",
    "\n",
    "With this foundation, you can construct an arbitrarily accurate precise model\n",
    "\n",
    "Problem is that we need some parameters, but we can easily get these using gradient descent\n",
    "\n",
    "We have just derived deep learning.\n",
    "\n",
    "Everything from now on is ways to make it faster and need less data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
